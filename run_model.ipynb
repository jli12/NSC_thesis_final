{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35264861",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc8cf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device\n",
      "Using 'cuda' device\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "# from statistics import geometric_mean\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "from itertools import accumulate\n",
    "\n",
    "from scipy.stats import gmean\n",
    "from task2 import generate_trials\n",
    "from ml_decorr import decorr_criterion\n",
    "from model import Model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b92b6",
   "metadata": {},
   "source": [
    "## misc helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdcc290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create population vector for performance calculation\n",
    "def popvec(y):\n",
    "    \"\"\"Population vector read out.\"\"\"\n",
    "    pref = np.arange(0, 2 * np.pi, 2 * np.pi / y.shape[-1])  # preferences\n",
    "    temp_sum = y.sum(axis=-1)\n",
    "    temp_cos = np.sum(y * np.cos(pref), axis=-1) / temp_sum\n",
    "    temp_sin = np.sum(y * np.sin(pref), axis=-1) / temp_sum\n",
    "    loc = np.arctan2(temp_sin, temp_cos)\n",
    "    return np.mod(loc, 2 * np.pi)\n",
    "\n",
    "\n",
    "# get model performance\n",
    "def get_perf(y_hat, y_loc):\n",
    "\n",
    "    if len(y_hat.shape) != 3:\n",
    "        raise ValueError('y_hat must have shape (Time, Batch, Unit)')\n",
    "    # Only look at last time points\n",
    "    y_loc = y_loc[-1]\n",
    "    y_hat = y_hat[-1]\n",
    "\n",
    "    # Fixation and location of y_hat\n",
    "    y_hat_fix = y_hat[..., 0]\n",
    "    y_hat_loc = popvec(y_hat[..., 1:])\n",
    "\n",
    "    # Fixating? Correctly saccading?\n",
    "    fixating = y_hat_fix > 0.5\n",
    "\n",
    "    original_dist = y_loc - y_hat_loc\n",
    "    corr_loc = ( 2*np.pi - abs(original_dist) ) / 2*np.pi\n",
    "    # dist = np.minimum(abs(original_dist), 2*np.pi-abs(original_dist))\n",
    "    # corr_loc = dist < 0.2*np.pi\n",
    "\n",
    "    # Should fixate?\n",
    "    should_fix = y_loc < 0\n",
    "\n",
    "    # performance\n",
    "    perf = should_fix * fixating + (1-should_fix) * corr_loc * (1-fixating)\n",
    "    return perf\n",
    "\n",
    "\n",
    "# generate orthogonal matrix for weight init\n",
    "def gen_ortho_matrix(dim, rng=None):\n",
    "    \"\"\"Generate random orthogonal matrix\n",
    "    Taken from scipy.stats.ortho_group\n",
    "    Copied here from compatibilty with older versions of scipy\n",
    "    \"\"\"\n",
    "    H = np.eye(dim)\n",
    "    for n in range(1, dim):\n",
    "        if rng is None:\n",
    "            x = np.random.normal(size=(dim-n+1,))\n",
    "        else:\n",
    "            x = rng.normal(size=(dim-n+1,))\n",
    "        # random sign, 50/50, but chosen carefully to avoid roundoff error\n",
    "        D = np.sign(x[0])\n",
    "        x[0] += D*np.sqrt((x*x).sum())\n",
    "        # Householder transformation\n",
    "        Hx = -D*(np.eye(dim-n+1) - 2.*np.outer(x, x)/(x*x).sum())\n",
    "        mat = np.eye(dim)\n",
    "        mat[n-1:, n-1:] = Hx\n",
    "        H = np.dot(H, mat)\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31df6ba",
   "metadata": {},
   "source": [
    "## model running funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306fdea7",
   "metadata": {},
   "source": [
    "#### pruning func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89da9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_layer_pruning(model, region, amount=0.2):\n",
    "    if region=='ctx': \n",
    "        prune.l1_unstructured(model.h2h_ctx, name='weight', amount=amount)\n",
    "        prune.l1_unstructured(model.ctx2out, name='weight', amount=amount)\n",
    "        \n",
    "    elif region=='hpc':\n",
    "        prune.l1_unstructured(model.h2h_hpc, name='weight', amount=amount)\n",
    "        prune.l1_unstructured(model.hpc2out, name='weight', amount=amount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89371e",
   "metadata": {},
   "source": [
    "#### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a619b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_interleaved(model, hp, task_list_unique, task_list, \n",
    "                      epoch_list=None, task_probs=None, save_hids=False, save_weights=False):\n",
    "\n",
    "    all_train_losses = []; all_train_perfs = []\n",
    "    all_losses_hpc = []\n",
    "    all_eval_losses = []; all_eval_perfs = []\n",
    "\n",
    "    all_hids_ctx = []; all_hids_hpc = []\n",
    "\n",
    "    all_ctx_h2h_weights = []; all_hpc_h2h_weights = []\n",
    "    all_ctx2hpc_weights = []; all_hpc2ctx_weights = []\n",
    "    all_ctx2out_weights = []; all_hpc2out_weights = []\n",
    "\n",
    "    all_yhats = []; all_eval_yhats = []\n",
    "    all_trials = []; all_eval_trials = []\n",
    "\n",
    "    batch_size = hp['batch_size']\n",
    "\n",
    "    mixed_batch_tasks = []\n",
    "\n",
    "    # print(f\"{task_list_unique} for {epoch_list} epochs\")\n",
    "    # print(f\"{hp['eval_epochs']} eval epoch per {hp['eval_step']} training epochs\")\n",
    "    # print(f\"{np.sum(epoch_list)} total epochs\")\n",
    "\n",
    "    ## --- sort out mixed batching --- ##\n",
    "    if hp['mixed_batch']: \n",
    "        if task_probs==None: raise Exception(\"ERROR: NO TASK PROBABILITIES\")\n",
    "        epoch_intervals = task_probs.keys()\n",
    "        epoch_accum = list(accumulate([int(a[:-1]) for a in epoch_intervals]))\n",
    "        train_paradigm = 0\n",
    "        task_list = ['hi']*epoch_accum[-1] # filler list for iterating over\n",
    "\n",
    "        hp['batch_size']=1 # changed to 1 since each trial within the mixed batch is singular\n",
    "        task_probs['task_list_unique'] = task_list_unique\n",
    "        hp['task_probs'] = task_probs\n",
    "\n",
    "    # print(len(task_list))\n",
    "    i = 0\n",
    "    for i, task in tqdm(enumerate(task_list)):\n",
    "\n",
    "        ## --- task generation --- ##\n",
    "        if hp['mixed_batch']: \n",
    "            if i==epoch_accum[train_paradigm]: train_paradigm+=1\n",
    "\n",
    "            task = random.choices(task_list_unique, task_probs[list(epoch_intervals)[train_paradigm]], k=batch_size)\n",
    "            hp['tdim'] = random.randint(75, 125) # min and max tdims here\n",
    "            mixed_batch_tasks.append(task)\n",
    "\n",
    "        trial = generate_trials(task, hp, 'random', batch_size=hp['batch_size'])\n",
    "        all_trials.append(trial)\n",
    "        inputs = torch.tensor(trial.x, dtype=torch.float32).to(device)\n",
    "        mask = torch.tensor(trial.c_mask, dtype=torch.float32).to(device)\n",
    "        y = torch.tensor(trial.y, dtype=torch.float32).to(device)\n",
    "\n",
    "        ctx_hid = torch.zeros(batch_size, hp['hid_size_ctx']).to(device)\n",
    "        hpc_hid = torch.zeros(batch_size, hp['hid_size_hpc']).to(device)\n",
    "\n",
    "        yhats = []\n",
    "        outputs = []\n",
    "        ctx_hids = []\n",
    "        hpc_hids = []\n",
    "\n",
    "        loss = 0 # full loss\n",
    "        loss_hpc = 0 # just hpc loss\n",
    "\n",
    "\n",
    "        ## --- training step --- ##\n",
    "        model.train()\n",
    "        model.optimizer.zero_grad()\n",
    "\n",
    "        n_timesteps = inputs.shape[0]\n",
    "        for t in range(n_timesteps):\n",
    "\n",
    "            (y_hat), ctx_hid, hpc_hid  = model(inputs[t], ctx_hid, hpc_hid) # .to(device)\n",
    "            \n",
    "            if hp['hpc_loss'] != None: \n",
    "                if hp['hpc_loss']=='recon': \n",
    "                    y_hat, reconstruction = y_hat\n",
    "                    l_hpc = model.reconstruction_loss(inputs[t], reconstruction)\n",
    "                elif hp['hpc_loss']=='decorr': \n",
    "                    l_hpc = decorr_criterion(hpc_hid)\n",
    "                loss_hpc += l_hpc\n",
    "                \n",
    "            l = model.compute_loss(y[t], y_hat, mask[t*batch_size:(t+1)*batch_size])\n",
    "            loss += l\n",
    "\n",
    "            yhats.append(y_hat)\n",
    "            outputs.append(y_hat.cpu().detach().numpy())\n",
    "\n",
    "            if save_hids:\n",
    "                ctx_hids.append(ctx_hid.detach().cpu())\n",
    "                hpc_hids.append(hpc_hid.detach().cpu())\n",
    "\n",
    "        # outputs = torch.stack(outputs)\n",
    "        perf = np.mean(get_perf(np.asarray(outputs), trial.y_loc)) # [:,batch_idxs])) # .cpu().detach().numpy()\n",
    "\n",
    "        if hp['force_sparsity']:\n",
    "            l1_norm_h2h_ctx = model.h2h_ctx.weight.abs().sum()\n",
    "            l1_norm_h2h_hpc = model.h2h_hpc.weight.abs().sum()\n",
    "            l1_norm_ctx2out = model.ctx2out.weight.abs().sum()\n",
    "            l1_norm_hpc2out = model.hpc2out.weight.abs().sum()\n",
    "\n",
    "            loss += (hp['l1_lambda_ctx'] * l1_norm_h2h_ctx) + (hp['l1_lambda_hpc'] * l1_norm_h2h_hpc) + \\\n",
    "                    (hp['l1_lambda_ctx'] * l1_norm_ctx2out) + (hp['l1_lambda_hpc'] * l1_norm_hpc2out)\n",
    "\n",
    "\n",
    "        if hp['hpc_loss'] != None: \n",
    "            loss = loss + (hp['hpc_loss_alpha']*loss_hpc)\n",
    "            # loss = (hp['hpc_loss_alpha']*loss) + ((1-hp['hpc_loss_alpha'])*loss_hpc)\n",
    "            \n",
    "        loss.backward() # retain_graph=True\n",
    "        model.optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "\n",
    "        ## ---- prunning ---- ##\n",
    "        if hp['force_sparsity'] and i % hp['prune_int']==0 and i!=0:\n",
    "            apply_layer_pruning(model, 'ctx', amount=hp['prune_ctx'])\n",
    "            apply_layer_pruning(model, 'hpc', amount=hp['prune_hpc'])\n",
    "\n",
    "            prune.remove(model.h2h_ctx, 'weight')\n",
    "            prune.remove(model.h2h_hpc, 'weight')\n",
    "            prune.remove(model.ctx2out, 'weight')\n",
    "            prune.remove(model.hpc2out, 'weight')\n",
    "\n",
    "        ## ---- logging ---- ##\n",
    "        all_train_perfs.append(np.mean(perf))\n",
    "        all_train_losses.append(float(loss.detach().cpu()))\n",
    "        all_yhats.append(yhats)\n",
    "\n",
    "        if hp['hpc_loss'] != None: \n",
    "            all_losses_hpc.append(float(loss_hpc.detach().cpu()))\n",
    "\n",
    "\n",
    "        if save_hids:\n",
    "            all_hids_ctx.append(ctx_hids)\n",
    "            all_hids_hpc.append(hpc_hids)\n",
    "\n",
    "        if save_weights and i % hp['weight_save_int'] == 0: #  and i!=0:\n",
    "            ctx_h2h_weights = model.state_dict()['h2h_ctx.weight'].cpu().detach().numpy()\n",
    "            hpc_h2h_weights = model.state_dict()['h2h_hpc.weight'].cpu().detach().numpy()\n",
    "            ctx2hpc_weights = model.state_dict()['ctx2hpc.weight'].cpu().detach().numpy()\n",
    "            hpc2ctx_weights = model.state_dict()['hpc2ctx.weight'].cpu().detach().numpy()\n",
    "            ctx2out_weights = model.state_dict()['ctx2out.weight'].cpu().detach().numpy()\n",
    "            hpc2out_weights = model.state_dict()['hpc2out.weight'].cpu().detach().numpy()\n",
    "\n",
    "            all_ctx_h2h_weights.append(ctx_h2h_weights)\n",
    "            all_hpc_h2h_weights.append(hpc_h2h_weights)\n",
    "            all_ctx2hpc_weights.append(ctx2hpc_weights)\n",
    "            all_hpc2ctx_weights.append(hpc2ctx_weights)\n",
    "            all_ctx2out_weights.append(ctx2out_weights)\n",
    "            all_hpc2out_weights.append(hpc2out_weights)\n",
    "\n",
    "        \n",
    "        ## ---- eval step --- ## \n",
    "        # if epoch % eval_step == 0: # ylocs, ys, xs, \n",
    "        eval_perfs, eval_losses, yhats, eval_trial, eval_hids_ctx, eval_hids_hpc = run_eval(model, hp, task_list_unique)\n",
    "\n",
    "        all_eval_perfs.append(eval_perfs)\n",
    "        all_eval_losses.append(eval_losses)\n",
    "\n",
    "        all_eval_trials.append(eval_trial)\n",
    "        all_eval_yhats.append(yhats)\n",
    "        # all_xs.append(xs)\n",
    "        # all_ylocs.append(ylocs)\n",
    "        # all_ys.append(ys)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return (all_train_losses, all_losses_hpc, all_train_perfs, mixed_batch_tasks), \\\n",
    "            (all_eval_losses, all_eval_perfs), \\\n",
    "            (all_yhats, all_trials, all_eval_yhats, all_eval_trials), \\\n",
    "            (all_hids_ctx, all_hids_hpc), \\\n",
    "            (all_ctx_h2h_weights, all_hpc_h2h_weights), \\\n",
    "            (all_ctx2hpc_weights, all_hpc2ctx_weights), \\\n",
    "            (all_ctx2out_weights, all_hpc2out_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168e13d",
   "metadata": {},
   "source": [
    "#### eval func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0255b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EVAL FUNC\n",
    "\n",
    "def run_eval(model, hp, task_list):\n",
    "    model.eval()\n",
    "    \n",
    "    batch_size = hp['eval_batch_size']\n",
    "    device = hp['device']\n",
    "    \n",
    "    all_hid_ctx = []\n",
    "    all_hid_hpc = []\n",
    "\n",
    "    all_perfs = []\n",
    "    all_losses = []\n",
    "\n",
    "    all_yhats = []\n",
    "    all_trials = []\n",
    "\n",
    "\n",
    "    for task in task_list:\n",
    "        task_hid_ctx = []\n",
    "        task_hid_hpc = []\n",
    "\n",
    "        task_perfs = []\n",
    "        task_losses = []\n",
    "\n",
    "        task_yhats = []\n",
    "        task_trials = []\n",
    "\n",
    "        epoch = 0 \n",
    "        for epoch in range(hp['eval_epochs']):\n",
    "\n",
    "            trial = generate_trials(task, hp, 'random', batch_size=batch_size)\n",
    "            inputs = torch.tensor(trial.x, dtype=torch.float32).to(device)\n",
    "            # y_loc = torch.tensor(trial.y_loc, dtype=torch.float32).to(device)\n",
    "            mask = torch.tensor(trial.c_mask, dtype=torch.float32).to(device)\n",
    "            y = torch.tensor(trial.y, dtype=torch.float32).to(device)\n",
    "\n",
    "            ctx_hid = torch.zeros(batch_size, hp['hid_size_ctx']).to(device)\n",
    "            hpc_hid = torch.zeros(batch_size, hp['hid_size_hpc']).to(device)\n",
    "        \n",
    "            yhats = []\n",
    "            loss = 0\n",
    "            n_timesteps = inputs.shape[0]\n",
    "            for t in range(n_timesteps):\n",
    "                (y_hat), ctx_hid, hpc_hid  = model(inputs[t], ctx_hid, hpc_hid) # .to(device)\n",
    "\n",
    "                l = model.compute_loss(y[t], y_hat, mask[t*batch_size:(t+1)*batch_size])\n",
    "                loss += l\n",
    "\n",
    "                yhats.append(y_hat.cpu().detach().numpy())\n",
    "                # outputs.append(y_hat.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "            perf = np.mean(get_perf(np.asarray(yhats), trial.y_loc)) # .cpu().detach().numpy()\n",
    "\n",
    "            task_perfs.append(np.mean(perf))\n",
    "            task_losses.append(float(loss.detach().cpu()))\n",
    "\n",
    "            if hp['save_eval_hids']:\n",
    "                task_hid_ctx.append(ctx_hid)\n",
    "                task_hid_hpc.append(hpc_hid)\n",
    "\n",
    "            task_yhats.append(yhats)\n",
    "            task_trials.append(trial)\n",
    "\n",
    "            epoch+= 1\n",
    "\n",
    "\n",
    "        all_perfs.append(task_perfs)\n",
    "        all_losses.append(task_losses)\n",
    "\n",
    "        all_yhats.append(task_yhats)\n",
    "        all_trials.append(task_trials)\n",
    "\n",
    "\n",
    "        if hp['save_eval_hids']:\n",
    "            all_hid_ctx.append(task_hid_ctx)\n",
    "            all_hid_hpc.append(task_hid_hpc)\n",
    "\n",
    "    return all_perfs, all_losses, all_yhats, all_trials, all_hid_ctx, all_hid_hpc # all_ylocs, all_ys, all_xs, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee288c",
   "metadata": {},
   "source": [
    "#### save data func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "886317ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(model, hp, task_list_unique, all_eval_perfs, all_eval_losses, \n",
    "              epoch_list=None,\n",
    "              all_trials=None, all_eval_trials=None,\n",
    "              all_yhats=None, all_eval_yhats=None,\n",
    "              all_train_perfs=None, all_train_losses=None, \n",
    "              all_losses_hpc=None, mixed_batch_tasks=None,\n",
    "              all_hids_ctx=None, all_hids_hpc=None, \n",
    "              all_ctx_h2h_weights=None, all_hpc_h2h_weights=None,\n",
    "              all_ctx2hpc_weights=None, all_hpc2ctx_weights=None, \n",
    "              all_ctx2out_weights=None, all_hpc2out_weights=None):\n",
    "\n",
    "    from datetime import datetime\n",
    "    import glob\n",
    "\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    results_dir = '/home/jason/dev/schema1/results'\n",
    "    # if hp['hpc_reconstruct']: results_dir += '/recon-loss'\n",
    "    n_tasks = len(task_list_unique)\n",
    "\n",
    "    ## --- create the name string based on tasks --- ##\n",
    "    model_name_str = ''\n",
    "    for i in range(n_tasks):\n",
    "        if hp['mixed_batch']: model_name_str = model_name_str + str(task_list_unique[i])\n",
    "        else: model_name_str = model_name_str + str(task_list_unique[i]) + str(epoch_list[i])\n",
    "        if i != (n_tasks-1):\n",
    "            model_name_str += '-'\n",
    "\n",
    "    ## --- add network size label  --- ##\n",
    "    model_name_str += '_ctx'+str(hp['hid_size_ctx']) + '-hpc'+str(hp['hid_size_hpc'])\n",
    "\n",
    "    ## --- add mixed batch label  --- ##\n",
    "    if 'mixed_batch' in hp and hp['mixed_batch']: \n",
    "        epoch_intervals = list(hp['task_probs'].keys())[:-1]\n",
    "        epoch_accum = list(accumulate([int(a[:-1]) for a in epoch_intervals]))\n",
    "        model_name_str += '_mixedbatch' + str(epoch_accum[-1]) + 'epochs'\n",
    "\n",
    "    ## --- add hpc loss label  --- ##\n",
    "    if hp['hpc_reconstruct']!=None: \n",
    "        if hp['hpc_loss']=='decorr': model_name_str += '_decorrloss' +'{:.0e}'.format(hp['hpc_loss_alpha'])\n",
    "        elif hp['hpc_loss']=='recon': model_name_str += '_reconloss' + str(hp['hpc_loss_alpha'])\n",
    "    if hp['force_sparsity']: model_name_str += '_sparse'\n",
    "    results_dir = os.path.join(results_dir, model_name_str)\n",
    "\n",
    "    try: os.mkdir(results_dir)\n",
    "    except: pass\n",
    "\n",
    "    ## --- get directory str  --- ##\n",
    "    this_dir = today + '_lrctx'+'{:.0e}'.format(hp['lr_ctx']) + '_lrhpc'+'{:.0e}'.format(hp['lr_hpc']) \\\n",
    "              + '_c2h'+'{:.1e}'.format(hp['lr_c2h']) + '_h2c'+'{:.1e}'.format(hp['lr_h2c']) \n",
    "\n",
    "    \n",
    "    if os.path.isdir(os.path.join(results_dir, this_dir)):\n",
    "        n_files = len(glob.glob(os.path.join(results_dir, this_dir+'*')))\n",
    "        this_dir = this_dir+'_v'+str(n_files+1)\n",
    "    this_dir = os.path.join(results_dir, this_dir)\n",
    "    os.mkdir(this_dir)\n",
    "    print(this_dir)\n",
    "\n",
    "\n",
    "    os.mkdir(os.path.join(this_dir, 'perfs'))\n",
    "    os.mkdir(os.path.join(this_dir, 'losses'))\n",
    "    os.mkdir(os.path.join(this_dir, 'hids'))\n",
    "    os.mkdir(os.path.join(this_dir, 'weights'))\n",
    "    os.mkdir(os.path.join(this_dir, 'trials'))\n",
    "    os.mkdir(os.path.join(this_dir, 'yhats'))\n",
    "\n",
    "    perfs_dir = os.path.join(this_dir, 'perfs')\n",
    "    losses_dir = os.path.join(this_dir, 'losses')\n",
    "    hids_dir = os.path.join(this_dir, 'hids')\n",
    "    weights_dir = os.path.join(this_dir, 'weights')\n",
    "    trials_dir = os.path.join(this_dir, 'trials')\n",
    "    yhats_dir = os.path.join(this_dir, 'yhats')\n",
    "\n",
    "\n",
    "    ## --- always save model  --- ##\n",
    "    model_save_path = os.path.join(this_dir, model_name_str + '.pth')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print('model saved at:' + model_save_path)\n",
    "\n",
    "    ## --- always hp dict  --- ##\n",
    "    with open(os.path.join(this_dir, 'hp.pkl'), 'wb') as f:\n",
    "                pkl.dump(hp, f)\n",
    "\n",
    "    rng_save = hp.pop('rng')\n",
    "    device_save = hp.pop('device')\n",
    "    json_object = json.dumps(hp, indent=4)\n",
    "    with open(os.path.join(this_dir, 'hp.json'), \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    hp['rng'] = rng_save; hp['device'] = device_save\n",
    "    print('hyperparameters saved')\n",
    "    \n",
    "\n",
    "    ## --- always perfs and losses  --- ##\n",
    "    with open(os.path.join(perfs_dir, 'eval_perfs.pkl'), 'wb') as f:\n",
    "                pkl.dump(all_eval_perfs, f) \n",
    "    with open(os.path.join(losses_dir, 'eval_losses.pkl'), 'wb') as f:\n",
    "                pkl.dump(all_eval_losses, f) \n",
    "    print(\"eval perfs and losses saved\")\n",
    "\n",
    "\n",
    "    ## --- save raw trials  --- ##\n",
    "    if all_trials!=None:\n",
    "        with open(os.path.join(trials_dir, 'train_trials.pkl'), 'wb') as f:\n",
    "                    pkl.dump(all_trials, f) \n",
    "        print(\"train trials saved\")\n",
    "    if all_eval_trials!=None:\n",
    "        with open(os.path.join(trials_dir, 'eval_trials.pkl'), 'wb') as f:\n",
    "                    pkl.dump(all_eval_trials, f) \n",
    "        print(\"eval trials saved\")\n",
    "\n",
    "    ## --- save yhats  --- ##\n",
    "    if all_yhats!=None:\n",
    "        with open(os.path.join(yhats_dir, 'train_yhats.pkl'), 'wb') as f:\n",
    "                    pkl.dump(all_yhats, f) \n",
    "        print(\"train yhats saved\")\n",
    "    if all_eval_yhats!=None:\n",
    "        with open(os.path.join(yhats_dir, 'eval_yhats.pkl'), 'wb') as f:\n",
    "                    pkl.dump(all_eval_yhats, f) \n",
    "        print(\"eval yhats saved\")\n",
    "\n",
    "    ## --- save train perfs and losses  --- ##\n",
    "    if all_train_perfs!=None: \n",
    "        with open(os.path.join(perfs_dir, 'train_perfs.pkl'), 'wb') as f:\n",
    "                    pkl.dump(all_train_perfs, f) \n",
    "        print(\"train perfs saved\")\n",
    "    if all_train_losses!=None:\n",
    "        with open(os.path.join(losses_dir, 'train_losses.pkl'), 'wb') as f:\n",
    "                    pkl.dump(all_train_losses, f) \n",
    "        print(\"train losses saved\")\n",
    "\n",
    "\n",
    "    ## --- save hpc loss  --- ##\n",
    "    if all_losses_hpc!=None:\n",
    "        with open(os.path.join(losses_dir, 'losses_sep.pkl'), 'wb') as f:\n",
    "                pkl.dump(all_losses_hpc, f)\n",
    "    ## if mixed batch\n",
    "    if hp['mixed_batch']: \n",
    "        with open(os.path.join(this_dir, 'mixed_batch_tasks.pkl'), 'wb') as f:\n",
    "                pkl.dump(mixed_batch_tasks, f)\n",
    "\n",
    "\n",
    "    ## --- save hids  --- ##\n",
    "    if all_hids_ctx!=None:\n",
    "        # with open(os.path.join(hids_dir, model_name_str + '_ctx-hids' + '.pkl'), 'wb') as f:\n",
    "        #             pkl.dump(all_hids_ctx, f) \n",
    "        with open(os.path.join(hids_dir, 'ctx-hids.pkl'), 'wb') as f:\n",
    "                    pkl.dump(all_hids_ctx, f) \n",
    "        with open(os.path.join(hids_dir, 'hpc-hids.pkl'), 'wb') as f:\n",
    "                    pkl.dump(all_hids_hpc, f)       \n",
    "        print(\"hids saved\")\n",
    "    \n",
    "\n",
    "    ## --- save weights  --- ##\n",
    "    if all_ctx_h2h_weights!= None:\n",
    "        with open(os.path.join(weights_dir, 'ctx-h2h-weights' + '.pkl'), 'wb') as f:\n",
    "                    pkl.dump(all_ctx_h2h_weights, f) \n",
    "        with open(os.path.join(weights_dir, 'hpc-h2h-weights' + '.pkl'), 'wb') as f:\n",
    "                    pkl.dump(all_hpc_h2h_weights, f)      \n",
    "        with open(os.path.join(weights_dir, 'ctx2hpc-weights' + '.pkl'), 'wb') as f:\n",
    "                    pkl.dump(all_ctx2hpc_weights, f) \n",
    "        with open(os.path.join(weights_dir, 'hpc2ctx-weights' + '.pkl'), 'wb') as f: \n",
    "                    pkl.dump(all_hpc2ctx_weights, f) \n",
    "        with open(os.path.join(weights_dir, 'ctx2out-weights' + '.pkl'), 'wb') as f: \n",
    "                    pkl.dump(all_ctx2out_weights, f) \n",
    "        with open(os.path.join(weights_dir, 'hpc2out-weights' + '.pkl'), 'wb') as f: \n",
    "                    pkl.dump(all_hpc2out_weights, f) \n",
    "        print(\"weights saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fcb03",
   "metadata": {},
   "source": [
    "#### load model func (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf2c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "\n",
    "if load_model:\n",
    "    model_dir = '/home/jason/dev/schema1/models/'\n",
    "    model_name = 'fdgo1000-reactgo1000-delayanti1000_a0.2_wd1e-05_ctx1e-04_hpc1e-03.pth'\n",
    "    model_load_path = os.path.join(model_dir, model_name)\n",
    "    model = torch.load(model_load_path, weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a6e9b",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c712f87",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rules_dict = \\\n",
    "    {'all' : ['fdgo', 'reactgo', 'delaygo', 'fdanti', 'reactanti', 'delayanti',\n",
    "              'dm1', 'dm2', 'contextdm1', 'contextdm2', 'multidm',\n",
    "              'delaydm1', 'delaydm2', 'contextdelaydm1', 'contextdelaydm2', 'multidelaydm',\n",
    "              'dmsgo', 'dmsnogo', 'dmcgo', 'dmcnogo'],\n",
    "\n",
    "    'mante' : ['contextdm1', 'contextdm2'],\n",
    "\n",
    "    'oicdmc' : ['oic', 'dmc']\n",
    "\t}\n",
    "\n",
    "ruleset = 'all'\n",
    "num_ring = 2\n",
    "n_eachring = 32\n",
    "n_rule = len(rules_dict[ruleset])\n",
    "\n",
    "in_size_model, out_size = 1+(num_ring*n_eachring)+n_rule, n_eachring+1\n",
    "n_input, n_output = 1+(num_ring*n_eachring)+n_rule, n_eachring+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HP DICT \n",
    "\n",
    "hp = {\n",
    "        'in_size_model': in_size_model,\n",
    "\t\t'out_size': out_size,\n",
    "        'n_eachring': n_eachring,\n",
    "\n",
    "        'hid_size_ctx': 64,\n",
    "        'hid_size_hpc': 64,       \n",
    "\n",
    "\t\t'learning_rate': 0.001, # optimizer lr\n",
    "        'lr_ctx': 1e-4,\n",
    "        'lr_hpc': 1e-3, \n",
    "\t\t# 'lr_c2h': gmean([hp['lr_ctx'], hp['lr_hpc']]),\n",
    "\t\t# 'lr_h2c': gmean([hp['lr_ctx'], hp['lr_hpc']]),\n",
    "\n",
    "        'activation': 'relu', # 'softplus', 'tanh', 'relu\n",
    "        'alpha': 0.2, # for forward pass / rnn gating\n",
    "\t\t'weight_decay': 1e-4,\n",
    "\n",
    "        'batch_size': 128,\n",
    "\t\t'eval_batch_size': 8,\n",
    "\t\t'eval_step': 1,\n",
    "\t\t'eval_epochs': 1,\n",
    "        'weight_save_int': 100, \n",
    "\n",
    "        'dt': 20,\n",
    "        'in_type': 'normal',\n",
    "        'loss_type': 'lsq',\n",
    "        'n_input': n_input,\n",
    "        'n_output': n_output,\n",
    "        'num_ring': num_ring,\n",
    "        'optimizer': 'adam',\n",
    "        'ruleset': ruleset,\n",
    "        'rule_start': 1+num_ring*n_eachring,\n",
    "        'save_name': 'test',\n",
    "        'sigma_rec': 0.05, # for noise calc\n",
    "        'sigma_x': 0.01, # for noise calc\n",
    "        'target_perf': 1.,\n",
    "        'tau': 100,\n",
    "        'use_separate_input': False,\n",
    "        'w_rec_init': 'randortho',\n",
    "        }\n",
    "\n",
    "hp['lr_c2h'] = gmean([hp['lr_ctx'], hp['lr_hpc']]) # hp['lr_ctx'] # gmean([hp['lr_ctx'], hp['lr_hpc']])\n",
    "hp['lr_h2c'] = gmean([hp['lr_ctx'], hp['lr_hpc']]) # hp['lr_hpc'] # hp['lr_c2h']\n",
    "\n",
    "# _w_in_start = 1.0\n",
    "# _w_rec_start = 0.5\n",
    "\n",
    "seed = 0\n",
    "trainables = 'all'\n",
    "\n",
    "hp['save_eval_hids'] = False\n",
    "hp['seed'] = seed\n",
    "hp['rng'] = np.random.RandomState(seed)\n",
    "hp['device'] = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "107feed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPARSITY VARS\n",
    "\n",
    "hp['force_sparsity'] = False\n",
    "\n",
    "hp['prune_int'] = 100\n",
    "hp['prune_ctx'] = 0.1\n",
    "hp['prune_hpc'] = 0.2\n",
    "hp['l1_lambda_ctx'] = 1e-5\n",
    "hp['l1_lambda_hpc'] = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HPC LOSS VARS\n",
    "\n",
    "hp['hpc_loss'] =  'decorr' # 'decorr' 'recon', None\n",
    "\n",
    "hp['hpc_loss_alpha'] = 1\n",
    "decorr_alpha = 1e-4 # 1e-3\n",
    "recon_alpha = 0.5\n",
    "\n",
    "if hp['hpc_loss']!=None : \n",
    "    if hp['hpc_loss']=='decorr': \n",
    "        hp['mixed_batch'] = True\n",
    "        hp['hpc_loss_alpha'] = decorr_alpha\n",
    "    elif hp['hpc_loss']=='recon': hp['hpc_loss_alpha'] = recon_alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b545be",
   "metadata": {},
   "source": [
    "### initiate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825904d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL RUNS:  1\n",
      "\n",
      "Training run #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:03,  1.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 105\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# if training_type == 'sequential':\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#     (all_train_losses, all_train_perfs), (all_eval_losses, all_eval_perfs), \\\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#     (all_xs, all_ys, all_yhats, all_ylocs), \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#     (all_ctx2hpc_weights, all_hpc2ctx_weights) \\\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#         = train_sequential(model, hp, task_list, epoch_list, save_hids=save_hids, save_weights=save_weights)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterleaved\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     98\u001b[0m     (all_train_losses, all_losses_hpc, all_train_perfs, mixed_batch_tasks), \\\n\u001b[1;32m     99\u001b[0m     (all_eval_losses, all_eval_perfs), \\\n\u001b[1;32m    100\u001b[0m     (all_yhats, all_trials, all_eval_yhats, all_eval_trials), \\\n\u001b[1;32m    101\u001b[0m     (all_hids_ctx, all_hids_hpc), \\\n\u001b[1;32m    102\u001b[0m     (all_ctx_h2h_weights, all_hpc_h2h_weights), \\\n\u001b[1;32m    103\u001b[0m     (all_ctx2hpc_weights, all_hpc2ctx_weights), \\\n\u001b[1;32m    104\u001b[0m     (all_ctx2out_weights, all_hpc2out_weights) \\\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_interleaved\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_list_unique\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepoch_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_hids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_hids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# if hp['mixed_batch']: (all_train_losses, all_losses_hpc, all_train_perfs, mixed_batch_tasks) = train_metrics\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# else: (all_train_losses, all_losses_hpc, all_train_perfs) = train_metrics; mixed_batch_tasks = None\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid training type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 157\u001b[0m, in \u001b[0;36mtrain_interleaved\u001b[0;34m(model, hp, task_list_unique, task_list, epoch_list, task_probs, save_hids, save_weights)\u001b[0m\n\u001b[1;32m    152\u001b[0m     all_hpc2out_weights\u001b[38;5;241m.\u001b[39mappend(hpc2out_weights)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m## ---- eval step --- ## \u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# if epoch % eval_step == 0: # ylocs, ys, xs, \u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m eval_perfs, eval_losses, yhats, eval_trial, eval_hids_ctx, eval_hids_hpc \u001b[38;5;241m=\u001b[39m \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_list_unique\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m all_eval_perfs\u001b[38;5;241m.\u001b[39mappend(eval_perfs)\n\u001b[1;32m    160\u001b[0m all_eval_losses\u001b[38;5;241m.\u001b[39mappend(eval_losses)\n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m(model, hp, task_list)\u001b[0m\n\u001b[1;32m     43\u001b[0m n_timesteps \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_timesteps):\n\u001b[0;32m---> 45\u001b[0m     (y_hat), ctx_hid, hpc_hid  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_hid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhpc_hid\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# .to(device)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     l \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss(y[t], y_hat, mask[t\u001b[38;5;241m*\u001b[39mbatch_size:(t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size])\n\u001b[1;32m     48\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\n",
      "File \u001b[0;32m~/Applications/miniforge3/envs/schema/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniforge3/envs/schema/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/schema1/model.py:149\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x, ctx_hid, hpc_hid, mask)\u001b[0m\n\u001b[1;32m    146\u001b[0m hpc_hid \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha) \u001b[38;5;241m*\u001b[39m hpc_hid_prev \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m hpc_hid)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m## Output computation\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m!=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m layer\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m region\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctx\u001b[39m\u001b[38;5;124m'\u001b[39m: \n\u001b[1;32m    151\u001b[0m         ctx_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx2out(ctx_hid) \u001b[38;5;241m*\u001b[39m lesion_mask\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") # if torch.cuda.is_available() else \"cpu\")\n",
    "save_all = True\n",
    "save_hids = False\n",
    "save_weights = True\n",
    "\n",
    "training_type = 'interleaved'\n",
    "hp['mode'] = 'random'\n",
    "hp['mixed_batch'] = True\n",
    "\n",
    "hp['hid_size_ctx'] = 64\n",
    "hp['hid_size_hpc'] = 64\n",
    "\n",
    "# print('mixed batch: ', hp['mixed_batch'], ' | hpc_loss: ', hp['hpc_loss'])\n",
    "\n",
    "\n",
    "# if training_type == 'sequential':\n",
    "#     task_list = ['reactgo', 'fdgo'] # , 'fdanti', 'reactanti'] # , 'reactanti_0', 'fdgo_0'] # , 'reactanti', 'dmsgo'] # delaygo\n",
    "#     task_list_unique = set(task_list)\n",
    "#     epoch_list = [4000, 4000, 2000] # , 1000, 1000] # , 1000, 500]\n",
    "\n",
    "if training_type == 'interleaved':\n",
    "\n",
    "    if hp['mixed_batch']:\n",
    "        task_list_unique = ['fdgo','reactgo', 'delaygo', 'reactoffgo', 'reactoffanti'] # 'reactoffgo', \n",
    "        task_probs = {\n",
    "            '1000a':  [1/3, 1/3, 1/3, 0.0, 0.0], # \"pre-training\" | '# epochs': probs corresponding to task_list\n",
    "            '2000a':  [1/7, 1/7, 1/7, 2/7, 2/7],\n",
    "        }\n",
    "\n",
    "        epoch_intervals = task_probs.keys()\n",
    "        epoch_accum = list(accumulate([int(a[:-1]) for a in epoch_intervals])) # getting the accumulation of epochs per paradigm switch\n",
    "        task_list = [0]*list(accumulate([int(a[:-1]) for a in epoch_intervals]))[-1]\n",
    "        epoch_list = None\n",
    "\n",
    "    else: ### --- tasks fully listed along with number of epochs, entire paradigm explicitly coded --- ### \n",
    "\n",
    "        # task_list_unique = ['fdgo','reactgo', 'delaygo', 'reactoffgo', 'reactoffanti'] \n",
    "        # epoch_list = [1000, 1000, 1000, 1000, 1000] # , 500,500] \n",
    "        # task_list_pre = [task_list_unique[0]]*500 + [task_list_unique[1]]*500 + [task_list_unique[2]]*500\n",
    "        # task_list_post = [task_list_unique[0]]*500 + [task_list_unique[1]]*500 + [task_list_unique[2]]*500 \\\n",
    "        #                  + [task_list_unique[3]]*1000 + [task_list_unique[4]]*1000\n",
    "        # random.shuffle(task_list_pre)\n",
    "        # random.shuffle(task_list_post)\n",
    "        # task_list = task_list_pre + task_list_post\n",
    "\n",
    "        task_list_unique = ['reactoffanti']\n",
    "        epoch_list = [500] # ,500,500]\n",
    "        task_list_pre = [task_list_unique[0]]*500\n",
    "        # task_list_post = ['fdgo']*500 + ['delaygo']*500\n",
    "        # random.shuffle(task_list_pre)\n",
    "        # random.shuffle(task_list_post)\n",
    "        task_list = task_list_pre #  + task_list_post\n",
    "\n",
    "        task_probs = None\n",
    "    \n",
    "else:\n",
    "    raise Exception(\"invalid training type\")\n",
    "\n",
    "# task_list_unique = set(task_list)\n",
    "# n_tasks_unique = len(task_list_unique)\n",
    "# total_n_epochs = np.sum(epoch_list)\n",
    "\n",
    "\n",
    "# scheduler = lr_scheduler.ExponentialLR(opt, gamma=0.99)\n",
    "n_tasks = len(task_list) # len(task_list) # 4\n",
    "n_runs = 1 # number of runs of this paradigm\n",
    "print(\"TOTAL RUNS: \", n_runs)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    \n",
    "    model = Model(hp).to(device)\n",
    "    opt = optim.Adam([ # setting the learning rates of the specific layers\n",
    "            {'params': model.params.cortical.parameters(), 'lr': hp['lr_ctx'],},\n",
    "            {'params': model.params.hippocampal.parameters(), 'lr': hp['lr_hpc'],},\n",
    "            {'params': model.params.ctx2hpc.parameters(), 'lr': hp['lr_c2h'],},\n",
    "            {'params': model.params.hpc2ctx.parameters(), 'lr': hp['lr_h2c'],},\n",
    "        ], weight_decay=hp['weight_decay']) # , betas=(0.9, 0.999)) # , momentum=0.9,)\n",
    "    model.set_optimizer(opt)\n",
    "    model.train()\n",
    "\n",
    "    print('\\nTraining run #'+str(i))\n",
    "\n",
    "    # if training_type == 'sequential':\n",
    "    #     (all_train_losses, all_train_perfs), (all_eval_losses, all_eval_perfs), \\\n",
    "    #     (all_xs, all_ys, all_yhats, all_ylocs), \\\n",
    "    #     (all_hids_ctx, all_hids_hpc), \\\n",
    "    #     (all_ctx_h2h_weights, all_hpc_h2h_weights), \\\n",
    "    #     (all_ctx2hpc_weights, all_hpc2ctx_weights) \\\n",
    "    #         = train_sequential(model, hp, task_list, epoch_list, save_hids=save_hids, save_weights=save_weights)\n",
    "\n",
    "    if training_type == 'interleaved':\n",
    "        (all_train_losses, all_losses_hpc, all_train_perfs, mixed_batch_tasks), \\\n",
    "        (all_eval_losses, all_eval_perfs), \\\n",
    "        (all_yhats, all_trials, all_eval_yhats, all_eval_trials), \\\n",
    "        (all_hids_ctx, all_hids_hpc), \\\n",
    "        (all_ctx_h2h_weights, all_hpc_h2h_weights), \\\n",
    "        (all_ctx2hpc_weights, all_hpc2ctx_weights), \\\n",
    "        (all_ctx2out_weights, all_hpc2out_weights) \\\n",
    "            = train_interleaved(model, hp, task_list_unique, task_list, \n",
    "                                epoch_list=epoch_list, task_probs=task_probs, save_hids=save_hids, save_weights=save_weights)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"invalid training type\")\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    if save_all:\n",
    "        save_data(model, hp, task_list_unique, all_eval_perfs, all_eval_losses, \n",
    "                    epoch_list=epoch_list, \n",
    "                    all_trials=all_trials, all_eval_trials=all_eval_trials,\n",
    "                    all_yhats=all_yhats, all_eval_yhats=all_eval_yhats,\n",
    "                    all_train_perfs=all_train_perfs, all_train_losses=all_train_losses,\n",
    "                    all_losses_hpc=all_losses_hpc, mixed_batch_tasks=mixed_batch_tasks,\n",
    "                    all_hids_ctx=all_hids_ctx, all_hids_hpc=all_hids_hpc, \n",
    "                    all_ctx_h2h_weights=all_ctx_h2h_weights, all_hpc_h2h_weights=all_hpc_h2h_weights,\n",
    "                    all_ctx2hpc_weights=all_ctx2hpc_weights, all_hpc2ctx_weights=all_hpc2ctx_weights,\n",
    "                    all_ctx2out_weights=all_ctx2out_weights, all_hpc2out_weights=all_hpc2out_weights)\n",
    "\n",
    "    print(\"ALL DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52073cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
